{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93d63f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dino_wm.utils.get_model import get_model\n",
    "from dino_wm.planning.objectives import create_objective_fn\n",
    "from dino_wm.planning.mpc import MPCPlanner\n",
    "from dino_wm.planning.cem import CEMPlanner\n",
    "from einops import rearrange, repeat\n",
    "import einops\n",
    "from dino_wm.utils.utils import move_to_device\n",
    "from dino_wm.env.venv import SubprocVectorEnv\n",
    "import gymnasium as gym\n",
    "import gym_pusht\n",
    "from gym_pusht.envs import PushTEnv\n",
    "import torch\n",
    "import numpy as np\n",
    "from dino_wm.models.visual_world_model import VWorldModel\n",
    "from dino_wm.utils.preprocessor import Preprocessor\n",
    "import imageio\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1785c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 1\n",
    "frameskip = 5\n",
    "horizon = 5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c96168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18685 rollouts\n",
      "Loaded 21 rollouts\n",
      "Resuming from epoch 2: /home/ianchuang/dino_wm/outputs/checkpoints/outputs/pusht/checkpoints/model_latest.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ianchuang/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_action_repeat: 1\n",
      "num_proprio_repeat: 1\n",
      "proprio encoder: ProprioceptiveEmbedding(\n",
      "  (patch_embed): Conv1d(4, 10, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "action encoder: ProprioceptiveEmbedding(\n",
      "  (patch_embed): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "proprio_dim: 10, after repeat: 10\n",
      "action_dim: 10, after repeat: 10\n",
      "emb_dim: 404\n",
      "Model emb_dim:  404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VWorldModel(\n",
       "  (encoder): DinoV2Encoder(\n",
       "    (base_model): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (proprio_encoder): ProprioceptiveEmbedding(\n",
       "    (patch_embed): Conv1d(4, 10, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (action_encoder): ProprioceptiveEmbedding(\n",
       "    (patch_embed): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (decoder): VQVAE(\n",
       "    (quantize_b): Quantize()\n",
       "    (upsample_b): Decoder(\n",
       "      (blocks): Sequential(\n",
       "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (4): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): ConvTranspose2d(384, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "        (8): ConvTranspose2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (dec): Decoder(\n",
       "      (blocks): Sequential(\n",
       "        (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (3): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (4): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): ReLU()\n",
       "            (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (5): ReLU(inplace=True)\n",
       "        (6): ConvTranspose2d(384, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "        (8): ConvTranspose2d(192, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predictor): ViTPredictor(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (transformer): Transformer(\n",
       "      (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (to_qkv): Linear(in_features=404, out_features=3072, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=404, bias=True)\n",
       "              (1): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): LayerNorm((404,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=404, out_features=2048, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Dropout(p=0.1, inplace=False)\n",
       "              (4): Linear(in_features=2048, out_features=404, bias=True)\n",
       "              (5): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder_criterion): MSELoss()\n",
       "  (emb_criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm, dataset, data_preprocessor = get_model(\"/home/ianchuang/dino_wm/outputs/checkpoints\", \"pusht\", device)\n",
    "wm : VWorldModel = wm\n",
    "wm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "828242cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym.vector.SyncVectorEnv\n",
    "env = gym.vector.AsyncVectorEnv(\n",
    "    [\n",
    "        lambda: gym.make(\n",
    "            \"gym_pusht/PushT-v0\", \n",
    "            disable_env_checker=True, \n",
    "            relative=True,\n",
    "            legacy=False,\n",
    "            obs_type=\"visual_proprio\", \n",
    "            render_mode=\"rgb_array\",\n",
    "            observation_width=224,\n",
    "            observation_height=224,\n",
    "        )\n",
    "        for _ in range(n_envs)\n",
    "    ]\n",
    ")\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c0721e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs['visual'] shape: torch.Size([25, 3, 224, 224])\n",
      "obs['proprio'] shape: torch.Size([25, 4])\n",
      "actions shape: torch.Size([25, 2])\n",
      "state shape: torch.Size([25, 7])\n",
      "start_obs['visual'] shape: torch.Size([1, 1, 3, 224, 224])\n",
      "start_obs['proprio'] shape: torch.Size([1, 1, 4])\n",
      "end_obs['visual'] shape: torch.Size([1, 1, 3, 224, 224])\n",
      "end_obs['proprio'] shape: torch.Size([1, 1, 4])\n",
      "actions shape: torch.Size([1, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "obs, actions, state, info = dataset[0]\n",
    "\n",
    "obs = {\n",
    "    k: v[-(frameskip*horizon):]\n",
    "    for k, v in obs.items()\n",
    "}\n",
    "actions = actions[-(frameskip*horizon):]\n",
    "state = state[-(frameskip*horizon):]\n",
    "print(f\"obs['visual'] shape: {obs['visual'].shape}\")\n",
    "print(f\"obs['proprio'] shape: {obs['proprio'].shape}\")\n",
    "print(f\"actions shape: {actions.shape}\")\n",
    "print(f\"state shape: {state.shape}\")\n",
    "\n",
    "start_obs = {}\n",
    "start_obs['visual'] = obs['visual'][:1].unsqueeze(0).repeat(n_envs, 1, 1, 1, 1)\n",
    "start_obs['proprio'] = obs['proprio'][:1].unsqueeze(0).repeat(n_envs, 1, 1)\n",
    "end_obs = {}\n",
    "end_obs['visual'] = obs['visual'][-1:].unsqueeze(0).repeat(n_envs, 1, 1, 1, 1)\n",
    "end_obs['proprio'] = obs['proprio'][-1:].unsqueeze(0).repeat(n_envs, 1, 1)\n",
    "print(f\"start_obs['visual'] shape: {start_obs['visual'].shape}\")\n",
    "print(f\"start_obs['proprio'] shape: {start_obs['proprio'].shape}\")\n",
    "print(f\"end_obs['visual'] shape: {end_obs['visual'].shape}\")\n",
    "print(f\"end_obs['proprio'] shape: {end_obs['proprio'].shape}\")\n",
    "\n",
    "actions = einops.rearrange(actions, \"(h f) a -> h (f a)\", f=frameskip, h=horizon).unsqueeze(0).repeat(n_envs, 1, 1)\n",
    "print(f\"actions shape: {actions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3fcc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverse_normalize(mean, std):\n",
    "    inv_std = [1.0/s for s in std]\n",
    "    inv_mean = [-m/s for m, s in zip(mean, std)]\n",
    "    return transforms.Normalize(mean=inv_mean, std=inv_std)\n",
    "inverse_normalize = get_inverse_normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "imageio.mimsave(\n",
    "    \"dataset.mp4\",\n",
    "    einops.rearrange(inverse_normalize(obs['visual']) * 255, \"b c h w -> b h w c\").cpu().numpy().astype(np.uint8),\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ad313cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "env.reset(options={\n",
    "    'reset_to_state': state[0],\n",
    "})\n",
    "env_actions = einops.rearrange(actions, \"b h (f a) -> b (h f) a\", f=frameskip)\n",
    "env_actions = data_preprocessor.denormalize_actions(env_actions) * 100.0\n",
    "for i in range(env_actions.shape[1]):\n",
    "    obs, reward, terminated, truncated, info = env.step(env_actions[:, i])\n",
    "    images.append(obs['visual'])\n",
    "\n",
    "imageio.mimsave(\n",
    "    \"env.mp4\",\n",
    "    einops.rearrange(np.array(images), \"n b h w c -> n h (b w) c\"),\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cedb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_obs = {\n",
    "    k: v.to(device)\n",
    "    for k, v in start_obs.items()\n",
    "}\n",
    "actions = actions.to(device)\n",
    "\n",
    "z_obs, z = wm.rollout(start_obs, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(0.5000, device='cuda:0', grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "wm_obs, diff = wm.decode_obs(z_obs)\n",
    "wm_images = wm_obs['visual'].squeeze(0)\n",
    "wm_images = torch.clamp(wm_images, 0, 1)\n",
    "\n",
    "imageio.mimsave(\n",
    "    \"imagination.mp4\",\n",
    "    einops.rearrange(inverse_normalize(wm_images) * 255, \"b c h w -> b h w c\").detach().cpu().numpy().astype(np.uint8),\n",
    "    fps=10 // 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_traj_segment_from_dset(self, traj_len):\n",
    "        states = []\n",
    "        actions = []\n",
    "        observations = []\n",
    "        env_info = []\n",
    "\n",
    "        # Check if any trajectory is long enough\n",
    "        valid_traj = [\n",
    "            self.dset[i][0][\"visual\"].shape[0]\n",
    "            for i in range(len(self.dset))\n",
    "            if self.dset[i][0][\"visual\"].shape[0] >= traj_len\n",
    "        ]\n",
    "        if len(valid_traj) == 0:\n",
    "            raise ValueError(\"No trajectory in the dataset is long enough.\")\n",
    "\n",
    "        # sample init_states from dset\n",
    "        for i in range(self.n_evals):\n",
    "            max_offset = -1\n",
    "            while max_offset < 0:  # filter out traj that are not long enough\n",
    "                traj_id = random.randint(0, len(self.dset) - 1)\n",
    "                obs, act, state, e_info = self.dset[traj_id]\n",
    "                max_offset = obs[\"visual\"].shape[0] - traj_len\n",
    "            state = state.numpy()\n",
    "            offset = random.randint(0, max_offset)\n",
    "            obs = {\n",
    "                key: arr[offset : offset + traj_len]\n",
    "                for key, arr in obs.items()\n",
    "            }\n",
    "            state = state[offset : offset + traj_len]\n",
    "            act = act[offset : offset + self.frameskip * self.goal_H]\n",
    "            actions.append(act)\n",
    "            states.append(state)\n",
    "            observations.append(obs)\n",
    "            env_info.append(e_info)\n",
    "        return observations, states, actions, env_info\n",
    "\n",
    "\n",
    "# update env config from val trajs\n",
    "observations, states, actions, env_info = (\n",
    "    self.sample_traj_segment_from_dset(traj_len=self.frameskip * self.goal_H + 1)\n",
    ")\n",
    "self.env.update_env(env_info)\n",
    "\n",
    "# get states from val trajs\n",
    "init_state = [x[0] for x in states]\n",
    "init_state = np.array(init_state)\n",
    "actions = torch.stack(actions)\n",
    "if self.goal_source == \"random_action\":\n",
    "    actions = torch.randn_like(actions)\n",
    "wm_actions = rearrange(actions, \"b (t f) d -> b t (f d)\", f=self.frameskip)\n",
    "exec_actions = self.data_preprocessor.denormalize_actions(actions)\n",
    "# replay actions in env to get gt obses\n",
    "rollout_obses, rollout_states = self.env.rollout(\n",
    "    self.eval_seed, init_state, exec_actions.numpy()\n",
    ")\n",
    "self.obs_0 = {\n",
    "    key: np.expand_dims(arr[:, 0], axis=1)\n",
    "    for key, arr in rollout_obses.items()\n",
    "}\n",
    "self.obs_g = {\n",
    "    key: np.expand_dims(arr[:, -1], axis=1)\n",
    "    for key, arr in rollout_obses.items()\n",
    "}\n",
    "self.state_0 = init_state  # (b, d)\n",
    "self.state_g = rollout_states[:, -1]  # (b, d)\n",
    "self.gt_actions = wm_actions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c98a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    k: np.expand_dims(v, axis=1).astype(np.float32)\n",
    "    for k, v in obs.items()\n",
    "}\n",
    "batch = data_preprocessor.transform_obs(batch)\n",
    "batch = {\n",
    "    k: v.to(device)\n",
    "    for k, v in batch.items()\n",
    "}\n",
    "\n",
    "z_obs_g = wm.encode_obs(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48703da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_wm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
